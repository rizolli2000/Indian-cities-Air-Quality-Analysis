# -*- coding: utf-8 -*-
"""Indian air quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eh4RResW55Fxp5q-7YXsPx6pBml5aDYR
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

from google.colab import files
uploaded = files.upload()

data=pd.read_csv('city_day.csv')

data

data.drop(axis=1,columns=['Date']).describe(include='all')

data.drop(axis=1,columns=['Date']).groupby(['City'])['AQI_Bucket'].agg(pd.Series.mode)

import seaborn as sns

data['Date']=pd.to_datetime(data['Date'])
data.Date.dt.year

pm2=data.groupby(['City',(data.Date.dt.year), (data.Date.dt.month)])['PM2.5'].mean()

data_mean_month=pd.DataFrame({'City':[i[0] for i in pm2.index],'Date':[i[1] for i in pm2.index],'month':[i[2] for i in pm2.index],'pm2.5':pm2.values})
data_mean_month

data_y=pd.to_datetime(data_mean_month['Date'].astype(str)+ '-'  + data_mean_month['month'].astype(int).astype(str)+'-1', format = '%Y-%m')

data_y

sns.lineplot(y=data_mean_month[data_mean_month['City']=='Visakhapatnam']['pm2.5'],x=data_y )

columns=list(set(list(data.columns))-set(['City','AQI_Bucket','Date']))

import matplotlib.pyplot as plt

for j in columns :
    plt.figure(figsize = (10, 5))
    plt.suptitle(j, fontsize=15)
    for x,i in enumerate(data.Date.dt.year.unique()):
        plt.title(str(i))
        #sort(data[data[data.Date.dt.year]==i]['PM2.5'])
        df=data[data.Date.dt.year==i].groupby(['City'])[j].max().sort_values(ascending=False).head(5) 
        plt.subplot(2, 3, 1+x)
        plt.subplots_adjust(left=0.1,
                        bottom=0.1,
                        right=0.9,
                        top=0.9,
                        wspace=0.4,
                        hspace=0.5)
        sns.barplot( x=df.values, y=df.index)

data[data['City']=='Ahmedabad']['AQI'].isna().mean()

sns.distplot(data[data['City']=='Ahmedabad']['AQI'])

data[data['City']=='Ahmedabad'].boxplot( column =['AQI'], grid = False)

data[data['City']=='Ahmedabad']['AQI'].describe()

data[data.Date.dt.year==2015]['AQI'].describe()

data['AQI'].groupby(data.Date.dt.year).count()

(data.groupby([data.Date.dt.year,'City'])['AQI'].count()>300).groupby(level=0).count()

data[data.Date.dt.year==2019].groupby(['City'])['AQI'].mean().sort_values(ascending=True)

sns.distplot(data[data['City']=='Delhi']['AQI'])

pd.options.display.max_rows = 100

(data.groupby(['City',data.Date.dt.year])['AQI'].mean()).groupby(level=0).pct_change().groupby(level=0).mean().sort_values(ascending=True)

pd.options.display.max_rows = 10
data.groupby(['City',data.Date.dt.year])['AQI'].mean()

(data[data.Date.dt.year!=2020].groupby(['City',data.Date.dt.year])['AQI'].mean()).pct_change().groupby(level=0).mean()
pd.options.display.max_rows = 100
(data[data.Date.dt.year!=2020].groupby(['City',data.Date.dt.year])['AQI'].mean()).groupby(level=0).pct_change().groupby(level=0).mean().sort_values(ascending=True)

data.groupby(['City',data.Date.dt.month])['AQI'].mean().groupby(level=0).idxmax()

y=data.groupby(['City',data.Date.dt.month])['AQI'].mean().groupby(level=0).idxmax()
count_month=[i[1] for i in y]
sns.countplot(count_month)

data_pre=data.copy()
data_pre['AQI_Bucket'].map({'Poor':2, 'Very Poor':1, 'Severe':0, 'Moderate':3, 'Satisfactory':4,
       'Good':5})

data_pre['AQI_Rank']=data_pre['AQI_Bucket'].map({'Poor':2, 'Very Poor':1, 'Severe':0, 'Moderate':3, 'Satisfactory':4,
       'Good':5})
data_pre

pd.options.display.max_rows = 1000

x=data_pre[data_pre['City']=='Ahmedabad'].groupby(['City','AQI_Bucket'])['AQI_Bucket'].count()
plt.pie(x/data_pre.groupby(['City'])['AQI_Bucket'].count(),labels=[i[1] for i in x.index])

y=data_pre[data_pre['AQI_Rank']<=2].groupby(['City'])['AQI_Rank'].count()/data_pre.groupby(['City'])['AQI_Bucket'].count()
y.dropna(inplace=True)
fig, ax = plt.subplots(figsize=(40, 15))
sns.barplot(x=y.sort_values().index,y=y.sort_values().values)

dataset=data_pre[(data_pre['City']=='Delhi') & (data_pre.Date.dt.year<2019  )]['AQI'].dropna()

dataset=np.array(dataset)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from tensorflow.keras.layers import Dropout

tf.random.set_seed(7)
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset=dataset.reshape(-1,1)
dataset = scaler.fit_transform(dataset)

train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
print(len(train), len(test))

def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)

look_back = 30
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.add(Dropout(0.2))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)

trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])
# calculate root mean squared error
trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
fig, ax = plt.subplots(figsize=(40, 10))
plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

data20=data_pre[(data_pre['City']=='Delhi') & (data_pre.Date.dt.year==2020  )]['AQI'].dropna()
dataset=np.array(data20)
dataset=dataset.reshape(-1,1)
dataset = scaler.fit_transform(dataset)

testX, testY = create_dataset(dataset, look_back)
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
# make predictions
testPredict = model.predict(testX)
# invert predictions
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])
# calculate root mean squared error
testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

dataset.shape

testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[29+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
fig, ax = plt.subplots(figsize=(40, 10))
plt.plot(scaler.inverse_transform(dataset))
plt.plot(testPredictPlot)
plt.show()

data20=data_pre[(data_pre['City']=='Delhi') & (data_pre.Date.dt.year==2020  ) &(data_pre.Date.dt.month==6)]['AQI'].dropna()
dataset=np.array(data20)
dataset

def predict(num_prediction, model):
    prediction_list = dataset[-look_back:]
    
    for _ in range(num_prediction):
        x = np.array(prediction_list[-look_back:])
        x = np.reshape(x,(1, 1, 30))
        out = model.predict(x)
       # print(scaler.inverse_transform(out))
        prediction_list = np.append(prediction_list, out)
    prediction_list = prediction_list[look_back-1:]
        
    return prediction_list

num_prediction = 365
forecast = predict(num_prediction, model)

fig, ax = plt.subplots(figsize=(40, 10))
plt.plot(scaler.inverse_transform(forecast.reshape(-1,1)))

time_series=pd.Series()

for i in data_pre['City'].unique():
    time_series.loc[i]=data_pre[data_pre['City']==i]['AQI']

k={}
for i in data_pre['City'].unique():
        k[i]=len(time_series.loc[i])

k=data_pre[data_pre['City']=='Visakhapatnam']['Date'].sort_values().head(1).reset_index(drop = True)[0].date()
print(k)

cities=[]
for i in data_pre['City'].unique():
    k=data_pre[data_pre['City']==i]['Date'].sort_values().head(1).reset_index(drop = True)[0].date()
    y=data_pre[data_pre['City']==i]['Date'].sort_values().tail(1).reset_index(drop = True)[0].date()
    if (y-k).days>1000:
        cities.append(i)
        print(i+' '+str(k)+' ' +str(y)+ ' '+str((y-k).days))

time_series=[]
for i in cities:
    data=data_pre[data_pre['City']==i][['Date','AQI']]
    city=pd.DataFrame(data, columns = ['Date', 'AQI'])
    city.set_index("Date",inplace=True)
    # set the date columns as index
    city.sort_index(inplace=True)
    all_times=pd.date_range(start='1/1/2015', end='7/1/2020')
    time_series.append(city.reindex(all_times))

final_cit=[]
remov=[]
for i,j in enumerate(time_series):
    if j['AQI'].isnull().mean()<0.3:
        print(cities[i],j['AQI'].isnull().mean())
        final_cit.append(cities[i])
    else:
        remov.append(i)
          
tim_series=[i for i in time_series if i['AQI'].isnull().mean()<0.3 ]  
len(tim_series)

for cit in cities:
    print(cit,data_pre[data_pre['City']==cit]['AQI'].isnull().mean())
final_cit

import math
def fill_null(x,y,i):
    a=tim_series[i].reset_index()
    return a[(a['index'].dt.month==x.month) & (a['index'].dt.day==x.day)]['AQI'].mean()
     
for i in range(len(tim_series)):
    
    if tim_series[i]['AQI'].isnull().sum()>0:
        a=tim_series[i].reset_index()
        b=a.apply(lambda x: fill_null(x['index'],x['AQI'],i) if math.isnan(x['AQI']) else x['AQI'],axis=1)
        all_times=pd.date_range(start='1/1/2015', end='7/1/2020')
        b.index=all_times
        tim_series[i]['AQI']=b

def fill_null(x,y,i):
    a=tim_series[i].reset_index()
    return a[(a['index'].dt.month==x.month) & (a['index'].dt.day==x.day)]['AQI'].mean()
     
for i in range(len(tim_series)):
    
    if tim_series[i]['AQI'].isnull().sum()>0:
        a=tim_series[i].reset_index()
        b=a.apply(lambda x: fill_null(x['index'],x['AQI'],i) if math.isnan(x['AQI']) else x['AQI'],axis=1)
        all_times=pd.date_range(start='1/1/2015', end='7/1/2020')
        b.index=all_times
        tim_series[i]['AQI']=b

sum=0
for i in range(len(tim_series)):
    sum+=tim_series[i]['AQI'].isnull().sum()
print(sum)

fig, axs = plt.subplots(5,2,figsize=(25,25))
fig.suptitle('Series')
for i in range(5):
    for j in range(2):
        if i*2+j+1>len(tim_series): # pass the others that we can't fill
            continue
        axs[i, j].plot(tim_series[i*2+j]['AQI'].values)
        axs[i, j].set_title(final_cit[i*2+j])
plt.show()

fig, axs = plt.subplots(5,2,figsize=(25,25))
fig.suptitle('Series')
for i in range(5):
    for j in range(2):
        if i*2+j+1>len(tim_series): # pass the others that we can't fill
            continue
        axs[i, j].boxplot(tim_series[i*2+j]['AQI'])
        axs[i, j].set_title(final_cit[i*2+j])
plt.show()

for i in range(len(tim_series)):
    Q1 = tim_series[i]['AQI'].quantile(0.25)
    Q3 = tim_series[i]['AQI'].quantile(0.75)
    IQR = Q3 - Q1
    print(final_cit[i],((tim_series[i]['AQI'] < (Q1 - 1.5 * IQR)) | (tim_series[i]['AQI'] > (Q3 + 1.5 * IQR))).sum())

for i in range(len(tim_series)):
    Q1 = tim_series[i]['AQI'].quantile(0.25)
    Q3 = tim_series[i]['AQI'].quantile(0.75)
    IQR = Q3 - Q1
    mask = (tim_series[i]['AQI'] < (Q1 - 1.5 * IQR)) | (tim_series[i]['AQI'] > (Q3 + 1.5 * IQR))
    tim_series[i]['AQI'][mask] = tim_series[i]['AQI'].median()

sns.distplot(tim_series[0])

tim_series[0].describe()

fig, axs = plt.subplots(5,2,figsize=(25,25))
fig.suptitle('Series')
for i in range(5):
    for j in range(2):
        if i*2+j+1>len(tim_series): # pass the others that we can't fill
            continue
        axs[i, j].plot(tim_series[i*2+j]['AQI'].values)
        axs[i, j].set_title(final_cit[i*2+j])
plt.show()

!pip install tslearn

tim_org=tim_series.copy()
tim_org

mySeries=tim_series
for i in range(len(mySeries)):
    scaler = MinMaxScaler()
    mySeries[i] = MinMaxScaler().fit_transform(mySeries[i])
tim_series=mySeries

import math
from tslearn.clustering import TimeSeriesKMeans
from sklearn.cluster import KMeans

from sklearn.decomposition import PCA
cluster_count = math.ceil(math.sqrt(len(tim_series))) 
# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN

km = TimeSeriesKMeans(n_clusters=cluster_count, metric="dtw",random_state=0)

labels = km.fit_predict(tim_series)

tim_series=tim_org

cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]
cluster_n = ["Cluster "+str(i) for i in range(cluster_count)]
plt.figure(figsize=(15,5))
plt.title("Cluster Distribution for KMeans")
plt.bar(cluster_n,cluster_c)
plt.show()

tim_series

fancy_names_for_labels = [f"Cluster {label}" for label in labels]
pd.DataFrame(zip(final_cit,fancy_names_for_labels),columns=["Series","Cluster"]).sort_values(by="Cluster").set_index("Series")

data=np.array([d.values for d in tim_series]) 
reshaped_data = data.reshape((len(tim_series),2009))
tim_T=[i.T for i in mySeries]
transf_series=np.concatenate(tim_T)

from sklearn.decomposition import PCA
pca = PCA(n_components=4)
mySeries_transformed = pca.fit_transform(transf_series)

mySeries_transformed.shape
mySeries=tim_series

kmeans = KMeans(n_clusters=cluster_count,max_iter=5000)

labels = kmeans.fit_predict(mySeries_transformed)

fancy_names_for_labels = [f"Cluster {label}" for label in labels]
pd.DataFrame(zip(final_cit,fancy_names_for_labels),columns=["Series","Cluster"]).sort_values(by="Cluster").set_index("Series")